{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frankiemacbook/opt/miniconda3/envs/py3/lib/python3.8/site-packages/geopandas/_compat.py:106: UserWarning: The Shapely GEOS version (3.8.0-CAPI-1.13.1 ) is incompatible with the GEOS version PyGEOS was compiled with (3.9.1-CAPI-1.14.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTables is not installed. No support for HDF output.\n",
      "SQLalchemy is not installed. No support for SQL output.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from simpledbf import Dbf5\n",
    "import pyxlsb\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from shapely.geometry import MultiLineString, LineString, Point\n",
    "import math\n",
    "import shapely\n",
    "\n",
    "import create_gtfs_from_basicinfo\n",
    "import sys, os\n",
    "import zipfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2018', '2026', '2031', '2036', '2041', '2051']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file = 'tests/test.shp'\n",
    "rootdir = '/Users/frankiemacbook/OneDrive - VicGov/VITM/GTFS/Ref case'\n",
    "\n",
    "fullpaths = map(lambda name: os.path.join(rootdir, name), os.listdir(rootdir))\n",
    "dirs = [x for x in fullpaths if os.path.isdir(x)]\n",
    "years = sorted([x.rsplit('/', 1)[-1] for x in dirs])\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "for year in years:\n",
    "    dbf = Dbf5(rootdir+'/'+year+'/PTLINK_ALL_Y'+year+'_VR19_Ref_C.DBF')\n",
    "    temp = dbf.to_dataframe()\n",
    "    temp.insert(0,'year',year)\n",
    "    data = data.append(temp, ignore_index=True)\n",
    "data = data[['PERIOD','A','B','VEHNAME','LINENO','LINKSEQ','STOPA','STOPB','NAME','LONGNAME','year']]\n",
    "data['YEAR_LINE_PERIOD'] = data['year'] + \"_\" + data['LINENO'].apply(str) + \"_\" + data['PERIOD']\n",
    "data['YEAR_LINE'] = data['year'] + \"_\" + data['LINENO'].apply(str)\n",
    "data['A_B'] = data['A'].apply(str) + \"_\" + data['B'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bus', 'skybus', 'train', 'tram', 'vline']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tram_names = data.loc[data['VEHNAME'].str.contains(\"^(?:Tram)\")]['VEHNAME'].unique().tolist()\n",
    "vline_names = data.loc[data['VEHNAME'].str.contains(\"^(?:V[Ll]ine)\")]['VEHNAME'].unique().tolist() + ['SP2']\n",
    "train_names = data.loc[(data['VEHNAME'].str.contains(\"^(?:Metro)\")) | \\\n",
    "                       (data['VEHNAME'].str.contains(\"^(?:HCMT)\")) | \\\n",
    "                       (data['VEHNAME'].str.contains(\"^(?:COMENG)\")) | \\\n",
    "                       (data['VEHNAME'].str.contains(\"^(?:SRL)\"))]['VEHNAME'].unique().tolist()\n",
    "\n",
    "replace_dict = {'Bus':'bus','SkyBus':'skybus'}\n",
    "for name in tram_names:\n",
    "    replace_dict[name] = 'tram'\n",
    "for name in vline_names:\n",
    "    replace_dict[name] = 'vline'\n",
    "for name in train_names:\n",
    "    replace_dict[name] = 'train'\n",
    "\n",
    "data['VEHNAME'] = data['VEHNAME'].replace(replace_dict)\n",
    "data = data.rename(columns = {'VEHNAME':'mode'})\n",
    "all_modes = sorted(data['mode'].unique().tolist())\n",
    "all_modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_fields_list = ['REGION','LINKC_IP','LINKC_PM','LINKC_OP', 'LANES_AM', 'LANES_IP', 'LANES_PM', 'LANES_OP', 'PSPD_AM', 'PSPD_IP', 'PSPD_PM', 'PSPD_OP', 'TOLLROAD', \\\n",
    "                      'TOLLGANT', 'TOLLENTRY', 'TOLLEXIT', 'TURN_BAN', 'CLEARWAY', 'ROADTYPE', 'RD_NAME', 'LASTPROJ', 'LX', 'LX_CODE', 'CROSSING', 'LXCLOSE', 'CTIME', \\\n",
    "                      'RAIL_SPD', 'TRAM_SPD3', 'BFLG_AM', 'BFLG_PM', 'SL', 'SL_CODE', 'TTR', 'TRN_CRDN', 'VLN_CRDN', 'TRM_CRDN', 'CRDN_DIR', 'FFSPD_AM', \\\n",
    "                      'FFSPD_IP', 'FFSPD_PM', 'FFSPD_OP', 'FFTIME_AM', 'FFTIME_IP', 'FFTIME_PM', 'FFTIME_OP', 'CSPD_AM', 'CSPD_IP', 'CSPD_PM', 'CSPD_OP', 'TIME_AM', \\\n",
    "                      'TIME_IP', 'TIME_PM', 'TIME_OP', 'TRSTIME_AM', 'TRSTIME_IP', 'TRSTIME_PM', 'TRSTIME_OP', 'TRMTIME_AM', 'TRMTIME_IP', 'TRMTIME_PM', 'TRMTIME_OP', \\\n",
    "                      'BUSTIME_AM', 'BUSTIME_IP', 'BUSTIME_PM', 'BUSTIME_OP', 'SMBTIME_AM', 'SMBTIME_IP', 'SMBTIME_PM', 'SMBTIME_OP', 'SKBTIME_AM', 'SKBTIME_IP', \\\n",
    "                      'SKBTIME_PM', 'SKBTIME_OP', 'SPBTIME_AM', 'SPBTIME_IP', 'SPBTIME_PM', 'SPBTIME_OP', 'VEH_AM', 'VEH_IP', 'VEH_PM', 'VEH_OP', 'VEH_WD', 'PVV_AM', \\\n",
    "                      'PVV_IP', 'PVV_PM', 'PVV_OP', 'PVV_WD', 'AP_EMP_AM', 'AP_EMP_IP', 'AP_EMP_PM', 'AP_EMP_OP', 'AP_EMP_WD', 'AP_PAS_AM', 'AP_PAS_IP', 'AP_PAS_PM', \\\n",
    "                      'AP_PAS_OP', 'AP_PAS_WD', 'TRUCK_AM', 'TRUCK_IP', 'TRUCK_PM', 'TRUCK_OP', 'TRUCK_WD', 'RIGID_AM', 'RIGID_IP', 'RIGID_PM', 'RIGID_OP', 'RIGID_WD', \\\n",
    "                      'ARTIC_AM', 'ARTIC_IP', 'ARTIC_PM', 'ARTIC_OP', 'ARTIC_WD', 'BDBLE_AM', 'BDBLE_IP', 'BDBLE_PM', 'BDBLE_OP', 'BDBLE_WD', 'HPFV_AM', 'HPFV_IP', \\\n",
    "                      'HPFV_PM', 'HPFV_OP', 'HPFV_WD', 'PCU_AM', 'PCU_IP', 'PCU_PM', 'PCU_OP', 'PCU_WD', 'VEH_VKT_AM', 'VEH_VKT_IP', 'VEH_VKT_PM', 'VEH_VKT_OP', \\\n",
    "                      'PVV_VKT_AM', 'PVV_VKT_IP', 'PVV_VKT_PM', 'PVV_VKT_OP', 'HCV_VKT_AM', 'HCV_VKT_IP', 'HCV_VKT_PM', 'HCV_VKT_OP', 'VEH_VHT_AM', 'VEH_VHT_IP', \\\n",
    "                      'VEH_VHT_PM', 'VEH_VHT_OP', 'PVV_VHT_AM', 'PVV_VHT_IP', 'PVV_VHT_PM', 'PVV_VHT_OP', 'HCV_VHT_AM', 'HCV_VHT_IP', 'HCV_VHT_PM', 'HCV_VHT_OP', \\\n",
    "                      'HYCAP_AM', 'HYCAP_IP', 'HYCAP_PM', 'HYCAP_OP', 'VC_AM', 'VC_IP', 'VC_PM', 'VC_OP', 'PT_AM', 'PT_IP', 'PT_PM', 'PT_OP', 'PT_WD', 'TRAIN_AM', \\\n",
    "                      'TRAIN_IP', 'TRAIN_PM', 'TRAIN_OP', 'TRAIN_WD', 'TRAM_AM', 'TRAM_IP', 'TRAM_PM', 'TRAM_OP', 'TRAM_WD', 'BUS_AM', 'BUS_IP', 'BUS_PM', 'BUS_OP', \\\n",
    "                      'BUS_WD', 'VLINE_AM', 'VLINE_IP', 'VLINE_PM', 'VLINE_OP', 'VLINE_WD', 'WLK_AE_AM', 'WLK_AE_IP', 'WLK_AE_PM', 'WLK_AE_OP', 'WLK_AE_WD', \\\n",
    "                      'PNR_AE_AM', 'PNR_AE_IP', 'PNR_AE_PM', 'PNR_AE_OP', 'PNR_AE_WD', 'PNR_VH_AM', 'PNR_VH_IP', 'PNR_VH_PM', 'PNR_VH_OP', 'PNR_VH_WD', 'TFTMBU_AM', \\\n",
    "                      'TFTMBU_IP', 'TFTMBU_PM', 'TFTMBU_OP', 'TFTMBU_WD', 'TFRLRL_AM', 'TFRLRL_IP', 'TFRLRL_PM', 'TFRLRL_OP', 'TFRLRL_WD', 'TRNNVEH_AM', 'TRNNVEH_IP', \\\n",
    "                      'TRNNVEH_PM', 'TRNNVEH_OP', 'TRMNVEH_AM', 'TRMNVEH_IP', 'TRMNVEH_PM', 'TRMNVEH_OP', 'BUSNVEH_AM', 'BUSNVEH_IP', 'BUSNVEH_PM', 'BUSNVEH_OP', \\\n",
    "                      'VLNNVEH_AM', 'VLNNVEH_IP', 'VLNNVEH_PM', 'VLNNVEH_OP', 'TRNCAP_AM', 'TRNCAP_IP', 'TRNCAP_PM', 'TRNCAP_OP', 'TRN_VC_AM', 'TRN_VC_IP', \\\n",
    "                      'TRN_VC_PM', 'TRN_VC_OP', 'TRMCAP_AM', 'TRMCAP_IP', 'TRMCAP_PM', 'TRMCAP_OP', 'TRM_VC_AM', 'TRM_VC_IP', 'TRM_VC_PM', 'TRM_VC_OP', 'BUSCAP_AM', \\\n",
    "                      'BUSCAP_IP', 'BUSCAP_PM', 'BUSCAP_OP', 'BUS_VC_AM', 'BUS_VC_IP', 'BUS_VC_PM', 'BUS_VC_OP', 'VLNCAP_AM', 'VLNCAP_IP', 'VLNCAP_PM', 'VLNCAP_OP', \\\n",
    "                      'VLN_VC_AM', 'VLN_VC_IP', 'VLN_VC_PM', 'VLN_VC_OP', 'PTCAP_AM', 'PTCAP_IP', 'PTCAP_PM', 'PTCAP_OP', 'PT_VC_AM', 'PT_VC_IP', 'PT_VC_PM', 'PT_VC_OP']\n",
    "\n",
    "# network = pd.DataFrame()\n",
    "\n",
    "#     temp = gpd.read_file(rootdir+'/'+year+'/SUMMARY_LOADED_NETWORK_LINKS_Y'+year+'_VR19_Ref_C.shp',ignore_fields=ignore_fields_list)\n",
    "#     temp = temp.loc[~temp['LINKC_AM'].isin([1,-1])]\n",
    "#     temp['A_B'] = temp['A'].apply(str) + \"_\" + temp['B'].apply(str)\n",
    "#     temp = temp.set_index('A_B')\n",
    "#     missing_index = temp.index.difference(network.index)\n",
    "#     network = network.append(temp.loc[missing_index, :])\n",
    "\n",
    "# for year in years[-2:]:\n",
    "network_year = years[-1]\n",
    "#     network = network.reset_index().rename(columns = {'index':'A_B'})\n",
    "network = gpd.read_file(rootdir+'/'+network_year+'/SUMMARY_LOADED_NETWORK_LINKS_Y'+network_year+'_VR19_Ref_C.shp',ignore_fields=ignore_fields_list)\n",
    "network = network.loc[~network['LINKC_AM'].isin([1,-1])]\n",
    "#     network = gpd.GeoDataFrame(network, geometry='geometry')\n",
    "network.crs=('EPSG:20255')\n",
    "network['DISTANCE'] = network['geometry'].length\n",
    "\n",
    "network['A_B'] = network['A'].apply(str) + \"_\" + network['B'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_reporting = pd.DataFrame()\n",
    "for year in years:\n",
    "    temp = pd.read_excel(rootdir+'/'+year+'/DetailedPTReporting_v200619_Y'+year+'_VR19_Ref_C.xlsb', sheet_name='Line Summary', engine='pyxlsb', skiprows=2)\n",
    "    temp.insert(0,'year',year)\n",
    "    pt_reporting = pt_reporting.append(temp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = data.merge(network[['DISTANCE','A_B','geometry']], how='left', on='A_B').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "segs = temp_data.groupby('A_B').first()[['index','mode','A','B','STOPA','STOPB','DISTANCE','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_links = segs.loc[segs['DISTANCE'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding missing nodes: 100.0% pairs processed                      \r"
     ]
    }
   ],
   "source": [
    "def find_shortest_path(row):\n",
    "    \n",
    "    if row['node_path'] is None:\n",
    "        \n",
    "        global rows_processed, segs_not_found\n",
    "        start_time = time.time()\n",
    "        too_long = False\n",
    "        bounding_nodes = [temp_data.loc[row['index']-1,'A'],temp_data.loc[row['index']+1,'B']]\n",
    "        mode = row['mode']\n",
    "        start_node = row['A']\n",
    "        end_node = row['B']\n",
    "        depth = 1\n",
    "        max_depth = 500\n",
    "        max_time = 90\n",
    "        node_tree = [[start_node]]\n",
    "        rows_processed +=1\n",
    "        print('Finding missing nodes: %.1f' % (100 * rows_processed / total_rows) + '% pairs processed                      ', end='\\r')\n",
    "        if row['next_nodes'] is not None:\n",
    "            node_tree = [node_tree[0] + row['next_nodes']]\n",
    "            max_depth = 500\n",
    "            max_time = 120\n",
    "        while True:\n",
    "            if (time.time() - start_time > 30) and too_long == False:\n",
    "                print('Node pair ('+str(start_node)+', '+str(end_node)+') is taking a long time to process', end='\\r')\n",
    "                too_long = True\n",
    "            new_tree = []\n",
    "            for nodes in node_tree:\n",
    "                try:\n",
    "                    if mode in ['train','vline']:\n",
    "                        new_nodes = network.loc[(network['A'] == nodes[-1]) & (network['LINKC_AM'] == 42), 'B'].to_list()\n",
    "                    else:\n",
    "                        new_nodes = network.loc[(network['A'] == nodes[-1]) & (network['LINKC_AM'] != 42), 'B'].to_list()\n",
    "                    if any(x == end_node for x in new_nodes):\n",
    "                        nodes.append(end_node)\n",
    "                        row['node_path'] = split_list(nodes)\n",
    "                        return row\n",
    "                    if (depth >= 5) and (len(new_nodes) > 2) and (random.random() < 0.5):\n",
    "                        new_nodes = []\n",
    "                    for new_node in new_nodes:\n",
    "                        if new_node not in nodes + bounding_nodes:\n",
    "                            nodes_copy = nodes.copy()\n",
    "                            nodes_copy.append(new_node)\n",
    "                            new_tree.append(nodes_copy)\n",
    "                except:\n",
    "                    None\n",
    "            node_tree = new_tree\n",
    "            depth += 1\n",
    "            if (depth == max_depth) or (len(node_tree) > 50000) or (time.time() - start_time > max_time):\n",
    "                segs_not_found.append('\"A\"='+str(start_node)+' OR \"B\"='+str(end_node))\n",
    "#                 print('Node pair ('+str(start_node)+', '+str(end_node)+') not found at depth='+str(depth)+' and tree length='+str(len(node_tree)), end='\\r')\n",
    "                row['node_path'] = None\n",
    "    return row\n",
    "\n",
    "def split_list(l):\n",
    "    new_list = []\n",
    "    for i in range(len(l)-1):\n",
    "        new_list.append((l[i],l[i+1]))\n",
    "    return new_list\n",
    "\n",
    "missing_links.insert(0,'node_path',None)\n",
    "missing_links.insert(0,'next_nodes',None)\n",
    "\n",
    "next_nodes = {'23237_23051':[23212,23191,23188],\n",
    "              '14610_23262':[23377,23375,23376],\n",
    "              '10322_19292':[30688,30682,30677],\n",
    "              '220608_44422':[220609,220610,220612],\n",
    "              '23051_23237':[23059,23060,23066],\n",
    "              '23262_14610':[23257,23253,23252],\n",
    "              '44422_220608':[220102,220103,220104],\n",
    "              '19292_10322':[30935,30937,30909],\n",
    "              '27950_26914':[27933,27928,27924],\n",
    "              '18386_233825':[279141,279140,279139],\n",
    "              '233825_18386':[279144,279145,279146],\n",
    "              '223580_15575':[223581,224430,224431]}\n",
    "\n",
    "for k in next_nodes:\n",
    "    if k in missing_links.index:\n",
    "        missing_links.at[k,'next_nodes'] = next_nodes[k]\n",
    "\n",
    "total_rows = missing_links.shape[0]\n",
    "rows_processed = 0\n",
    "\n",
    "segs_not_found = []\n",
    "missing_links = missing_links.apply(find_shortest_path, axis=1)\n",
    "if len(segs_not_found) > 0:\n",
    "    print(\"\\nSegments not found:\")\n",
    "    for s in segs_not_found:\n",
    "        print(s)\n",
    "        \n",
    "# import os\n",
    "# os.system(\"printf '\\a'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_stops(group):\n",
    "    group['STOPB'].iloc[:-1] = 0\n",
    "    group['STOPA'].iloc[1:] = 0\n",
    "    return group\n",
    "\n",
    "if missing_links.size != 0:\n",
    "    \n",
    "    ml_expand = missing_links[['index','A','B','STOPA','STOPB']] \\\n",
    "        .merge(missing_links.node_path.explode(), right_index = True, left_index = True) \\\n",
    "        .dropna() \\\n",
    "        .sort_values(['index','A','B']) \\\n",
    "        .reset_index(drop = True)\n",
    "\n",
    "    ml_expand['A_B'] = ml_expand['A'].apply(str) + \"_\" + ml_expand['B'].apply(str)\n",
    "\n",
    "    ml_expand = ml_expand.sort_values(['A_B','index'])\n",
    "    ml_expand = ml_expand.groupby('A_B').apply(pad_stops)\n",
    "\n",
    "    temp_data = temp_data.merge(ml_expand[['A_B','STOPA','STOPB','node_path']], how='left', on='A_B', suffixes=('','_y')).sort_values(['year','LINENO','PERIOD','LINKSEQ']).reset_index(drop=True)\n",
    "\n",
    "    temp_data[['new_A','new_B']] = pd.DataFrame(temp_data.loc[temp_data['node_path'].notnull(), 'node_path'].tolist(), index=temp_data.loc[temp_data['node_path'].notnull()].index)\n",
    "\n",
    "    temp_data['STOPA'] = np.where(temp_data['node_path'].notnull(), temp_data['STOPA_y'], temp_data['STOPA'])\n",
    "    temp_data['STOPB'] = np.where(temp_data['node_path'].notnull(), temp_data['STOPB_y'], temp_data['STOPB'])\n",
    "    temp_data['A'] = np.where(temp_data['node_path'].notnull(), temp_data['new_A'], temp_data['A'])\n",
    "    temp_data['B'] = np.where(temp_data['node_path'].notnull(), temp_data['new_B'], temp_data['B'])\n",
    "\n",
    "    temp_data = temp_data.drop(['node_path', 'new_A', 'new_B', 'STOPA_y', 'STOPB_y'], axis=1)\n",
    "\n",
    "    temp_data['A'] = temp_data['A'].apply(int)\n",
    "    temp_data['B'] = temp_data['B'].apply(int)\n",
    "\n",
    "    temp_data['A_B'] = temp_data['A'].apply(str) + \"_\" + temp_data['B'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = temp_data.drop(['index','DISTANCE','geometry'], axis=1).merge(network[['LINKC_AM','A_B','DISTANCE','BUS_SPD','geometry']], how='left', on='A_B').replace(np.nan,\"NaN\")\n",
    "\n",
    "df['LINKSEQ'] = df.groupby('YEAR_LINE_PERIOD').cumcount()\n",
    "df = df.merge(df.groupby('YEAR_LINE_PERIOD')['B'].apply(list).apply(lambda x: ','.join([str(y) for y in x])).reset_index().rename(columns={'B':'ROUTE_NODES_SEQ'}), how='left', on='YEAR_LINE_PERIOD')\n",
    "\n",
    "unique_routes = df.groupby('ROUTE_NODES_SEQ').first().reset_index().reset_index().rename(columns={'index':'UNIQUE_ROUTE'})[['ROUTE_NODES_SEQ','UNIQUE_ROUTE','YEAR_LINE_PERIOD']]\n",
    "\n",
    "df = df.merge(unique_routes.drop('YEAR_LINE_PERIOD',axis=1), how='left', on='ROUTE_NODES_SEQ').drop('ROUTE_NODES_SEQ',axis=1)\n",
    "df['UNIQUE_ROUTE_SEQ'] = df['UNIQUE_ROUTE'].apply(str) + '_' + df['LINKSEQ'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links_dict = {}\n",
    "\n",
    "def create_links_dict(feature):\n",
    "    global all_links_dict\n",
    "    if feature['A'] not in all_links_dict:\n",
    "        all_links_dict[feature['A']] = [feature['B']]\n",
    "    else:\n",
    "        if feature['B'] not in all_links_dict[feature['A']]:\n",
    "            all_links_dict[feature['A']].append(feature['B'])\n",
    "    if feature['B'] not in all_links_dict:\n",
    "        all_links_dict[feature['B']] = [feature['A']]\n",
    "    else:\n",
    "        if feature['A'] not in all_links_dict[feature['B']]:\n",
    "            all_links_dict[feature['B']].append(feature['A'])\n",
    "    return\n",
    "            \n",
    "df.drop_duplicates(subset=['A_B'])[['A','B']].apply(create_links_dict,axis=1)\n",
    "\n",
    "split_nodes = [node for node in all_links_dict if len(all_links_dict[node]) > 2]\n",
    "split_nodes = pd.Series(split_nodes)\n",
    "# df = df.merge(split_nodes, how='left', on='B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-e35a1f5779bf>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_unique['split_line'] = False\n",
      "<ipython-input-15-e35a1f5779bf>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_unique['first'] = False\n"
     ]
    }
   ],
   "source": [
    "def assert_route_and_linkc_termini(group):\n",
    "    group['LINKC_AM_next'] = group['LINKC_AM'].shift(-1)\n",
    "    group.loc[(group['LINKC_AM'] != group['LINKC_AM_next']) & \\\n",
    "              (((group['LINKC_AM'].isin([18,19,20,25])) & \\\n",
    "               (~group['LINKC_AM_next'].isin([18,19,20,25]))) | \\\n",
    "              ((~group['LINKC_AM'].isin([18,19,20,25])) & \\\n",
    "               (group['LINKC_AM_next'].isin([18,19,20,25])))),'split_line'] = True\n",
    "    group['B_next'] = group['B'].shift(-1)\n",
    "    group.loc[group['A'] == group['B_next'],'split_line'] = True\n",
    "    group['first'].iloc[0] = True\n",
    "    group['split_line'].iloc[-1] = True\n",
    "    group['STOPA'].iloc[0] = 1\n",
    "    group['STOPB'].iloc[-1] = 1\n",
    "    return group\n",
    "\n",
    "df_unique = df.loc[df['YEAR_LINE_PERIOD'].isin(unique_routes['YEAR_LINE_PERIOD'].to_list())]\n",
    "df_unique.loc[:,'split_line'] = False\n",
    "df_unique.loc[:,'first'] = False\n",
    "\n",
    "df_unique = df_unique.sort_values(['UNIQUE_ROUTE','LINKSEQ']) \\\n",
    "    .groupby('UNIQUE_ROUTE').apply(assert_route_and_linkc_termini).sort_values(['UNIQUE_ROUTE','LINKSEQ']) \\\n",
    "    .reset_index(drop=True)\n",
    "    \n",
    "df_unique = df_unique[['A_B','A','B','STOPA','STOPB','LINKC_AM','BUS_SPD','first','split_line','DISTANCE','UNIQUE_ROUTE','UNIQUE_ROUTE_SEQ','LINKSEQ','mode','geometry']]\n",
    "\n",
    "# df_unique.loc[((df_unique['B'].isin(split_nodes.to_list())) & (df_unique['mode'].isin(['bus','tram']))) | ((df_unique['STOPB'] == 1) & (df_unique['LINKC_AM'] == 42)),'split_line'] = True\n",
    "\n",
    "temp_split_nodes_B = df_unique.loc[((df_unique['split_line'] == True) & (df_unique['mode'].isin(['bus','tram']))) | ((df_unique['STOPB'] == 1) & (df_unique['LINKC_AM'] == 42)),'B']\n",
    "temp_split_nodes_A = df_unique.loc[df_unique['first'] == True,'A']\n",
    "split_nodes = split_nodes.append(temp_split_nodes_A.loc[~temp_split_nodes_A.isin(split_nodes.to_list())])\n",
    "split_nodes = split_nodes.append(temp_split_nodes_B.loc[~temp_split_nodes_B.isin(split_nodes.to_list())])\n",
    "df_unique.loc[df_unique['B'].isin(split_nodes.to_list()),'split_line'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-fe1bb5eafe52>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  split_line_rows['next_UNIQUE_ROUTE'] = split_line_rows['UNIQUE_ROUTE'].shift(-1)\n",
      "<ipython-input-16-fe1bb5eafe52>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  split_line_rows['prev_UNIQUE_ROUTE'] = split_line_rows['UNIQUE_ROUTE'].shift(1)\n",
      "<ipython-input-16-fe1bb5eafe52>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  split_line_rows['next_B'] = split_line_rows['B'].shift(-1)\n",
      "<ipython-input-16-fe1bb5eafe52>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  split_line_rows['prev_B'] = split_line_rows['B'].shift(1)\n"
     ]
    }
   ],
   "source": [
    "split_line_rows = df_unique.loc[df_unique['split_line'] == True].copy()\n",
    "split_line_rows.loc[:,'next_UNIQUE_ROUTE'] = split_line_rows.loc[:,'UNIQUE_ROUTE'].shift(-1)\n",
    "split_line_rows.loc[:,'prev_UNIQUE_ROUTE'] = split_line_rows.loc[:,'UNIQUE_ROUTE'].shift(1)\n",
    "split_line_rows.loc[:,'next_B'] = split_line_rows.loc[:,'B'].shift(-1)\n",
    "split_line_rows.loc[:,'prev_B'] = split_line_rows.loc[:,'B'].shift(1)\n",
    "\n",
    "found_A_equals_B = False\n",
    "first_links = False\n",
    "unique_A_equals_B = 0\n",
    "def check_start_end_nodes(row):\n",
    "    global found_A_equals_B, unique_A_equals_B, first_links\n",
    "    if found_A_equals_B:\n",
    "        row['A_equals_B'] = unique_A_equals_B\n",
    "    if row['split_line'] == True:\n",
    "        if (row['B'] == row['prev_B'] and \\\n",
    "            row['UNIQUE_ROUTE'] == row['prev_UNIQUE_ROUTE']) or \\\n",
    "            first_links:\n",
    "            row['A_equals_B'] = unique_A_equals_B\n",
    "            found_A_equals_B = False\n",
    "            first_links = False\n",
    "            unique_A_equals_B += 1\n",
    "        if row['B'] == row['next_B'] and \\\n",
    "            row['UNIQUE_ROUTE'] == row['next_UNIQUE_ROUTE']:\n",
    "            row['A_equals_B'] = -1\n",
    "            found_A_equals_B = True\n",
    "    elif row['first'] == True:\n",
    "        if row['A'] == row['B_first_split'] and \\\n",
    "            row['UNIQUE_ROUTE'] == row['UNIQUE_ROUTE_first_split']:\n",
    "            row['A_equals_B'] = unique_A_equals_B\n",
    "            found_A_equals_B = True\n",
    "            first_links = True\n",
    "    return row\n",
    "\n",
    "df_unique = df_unique.merge(split_line_rows[['UNIQUE_ROUTE','B','next_UNIQUE_ROUTE','prev_UNIQUE_ROUTE','next_B','prev_B']], \\\n",
    "                            how='left', left_index=True, right_index=True, suffixes=('','_first_split'))\n",
    "df_unique[['UNIQUE_ROUTE_first_split','B_first_split']] = df_unique[['UNIQUE_ROUTE_first_split','B_first_split']].backfill()\n",
    "df_unique['A_equals_B'] = -1\n",
    "df_unique = df_unique.apply(check_start_end_nodes, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_nodes_in_loop(group):\n",
    "    if group['A_equals_B'].iloc[0] != -1:\n",
    "        add_node_at = math.floor(group.shape[0] * 0.5)\n",
    "        group['split_line'].iloc[add_node_at] = True\n",
    "    return group\n",
    "\n",
    "df_unique = df_unique.groupby('A_equals_B').apply(add_nodes_in_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidating links and nodes: finished           \n"
     ]
    }
   ],
   "source": [
    "links = []\n",
    "stops = []\n",
    "nodes = []\n",
    "total_rows = df_unique.shape[0]\n",
    "rows_processed = 0\n",
    "\n",
    "def initial_consolidate_links_and_nodes(row):\n",
    "    global links, stops, nodes, rows_processed\n",
    "    a_node = row['A']\n",
    "    b_node = row['B']\n",
    "    stopa = 0\n",
    "    stopb = 0\n",
    "    dist = 0\n",
    "    alllinks = np.nan\n",
    "    allstops = np.nan\n",
    "    allnodes = np.nan\n",
    "    rows_processed +=1\n",
    "    print('Consolidating links and nodes: %.1f' % (100 * rows_processed / total_rows) + '% processed', end='\\r')\n",
    "    new_line = row['geometry']\n",
    "    line_slice = new_line.coords\n",
    "    if len(links) > 0:\n",
    "        line_slice = new_line.coords[1:]\n",
    "    links += [l for l in line_slice]\n",
    "    if len(nodes) == 0:\n",
    "        stops.append(Point(links[0]))\n",
    "        nodes.append(a_node)\n",
    "        stopa = 1\n",
    "    nodes.append(b_node)\n",
    "    if row['split_line'] == True:\n",
    "        alllinks = LineString(links)\n",
    "        dist = alllinks.length\n",
    "        stops.append(Point(links[-1]))\n",
    "        stopb = 1\n",
    "        allstops = stops.copy()\n",
    "        allnodes = nodes.copy()\n",
    "        links = []\n",
    "        stops = []\n",
    "        nodes = []\n",
    "    return [stopa, stopb, dist, alllinks, allstops, allnodes]\n",
    "\n",
    "df_unique[['STOPA','STOPB','SEGDIST','geometry','SEGSTOPS','SEGNODES']] = df_unique.apply(lambda row: pd.Series(initial_consolidate_links_and_nodes(row)), axis=1)\n",
    "\n",
    "print('Consolidating links and nodes: finished           ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df_unique.loc[df_unique['SEGSTOPS'].notnull()]\n",
    "\n",
    "df_unique['start_node'] = df_unique['SEGNODES'].apply(lambda nodes: nodes[0])\n",
    "df_unique['end_node'] = df_unique['SEGNODES'].apply(lambda nodes: nodes[-1])\n",
    "df_unique['start_point'] = df_unique['SEGSTOPS'].apply(lambda stops: stops[0])\n",
    "df_unique['end_point'] = df_unique['SEGSTOPS'].apply(lambda stops: stops[1])\n",
    "df_unique['unique_seg'] = df_unique['SEGNODES'].apply(lambda nodes: ','.join(str(node) for node in nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNIQUE_ROUTE</th>\n",
       "      <th>SEGNODES</th>\n",
       "      <th>LINKSEQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [UNIQUE_ROUTE, SEGNODES, LINKSEQ]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique.loc[df_unique['start_node'] == df_unique['end_node'],['UNIQUE_ROUTE','SEGNODES','LINKSEQ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpd.GeoDataFrame(df_unique[['start_node','end_node','geometry','mode','LINKC_AM','UNIQUE_ROUTE','LINKSEQ']],geometry='geometry').to_file('tests/df_unique.gpkg', driver='GPKG', crs='EPSG:20255')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df_unique.sort_values(['UNIQUE_ROUTE','LINKSEQ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_seg_to_split = df_unique.loc[(df_unique['mode'].isin(['bus','tram'])) & (~df_unique['LINKC_AM'].isin([18,19,20,25])) & (df_unique['BUS_SPD'] != 80)].groupby('unique_seg').first().reset_index()[['start_node','end_node','start_point','end_point','unique_seg','split_line','LINKC_AM','SEGDIST','BUS_SPD','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4628"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_nodes = unique_seg_to_split.drop_duplicates(subset=['start_node'])[['start_node','start_point']].rename(columns={'start_node':'node','start_point':'geometry'})\n",
    "temp_B_nodes = unique_seg_to_split.drop_duplicates(subset=['end_node'])[['end_node','end_point']].rename(columns={'end_node':'node','end_point':'geometry'})\n",
    "original_nodes = original_nodes.append(temp_B_nodes.loc[~temp_B_nodes['node'].isin(original_nodes['node'].to_list())])\n",
    "original_nodes = gpd.GeoDataFrame(original_nodes.reset_index(drop=True), geometry='geometry')\n",
    "\n",
    "original_nodes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_nodes.to_file('tests/original_nodes.gpkg', driver='GPKG', crs='EPSG:20255')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting long segments: finished           \n",
      "Split 9482 of 12675 segments\n"
     ]
    }
   ],
   "source": [
    "total_rows = unique_seg_to_split.shape[0]\n",
    "rows_processed = 0\n",
    "offset = 5\n",
    "count = 0\n",
    "\n",
    "def split_line(line, distance, segments):\n",
    "    line_segs = [list(line.coords)]\n",
    "    point = line.coords[0]\n",
    "    bbox = (point[0] - offset, point[1] - offset, point[0] + offset, point[1] + offset)\n",
    "    bboxes = [None]\n",
    "    for i in range(segments - 1):\n",
    "        coords = line_segs[-1]\n",
    "        line = LineString(coords)\n",
    "        line_segs = line_segs[:-1]\n",
    "        for i, p in enumerate(coords):\n",
    "            pd = line.project(Point(p))\n",
    "            if pd == distance:\n",
    "                point = coords[i]\n",
    "                bbox = (point[0] - offset, point[1] - offset, point[0] + offset, point[1] + offset)\n",
    "                line_segs += [coords[:i+1], coords[i:]]\n",
    "                bboxes[-1] = (bboxes[-1],bbox)\n",
    "                bboxes.append(bbox)\n",
    "                break\n",
    "            if pd > distance:\n",
    "                cp = line.interpolate(distance)\n",
    "                point = (cp.x, cp.y)\n",
    "                bbox = (point[0] - offset, point[1] - offset, point[0] + offset, point[1] + offset)\n",
    "                line_segs += [coords[:i] + [point],[point] + coords[i:]]\n",
    "                bboxes[-1] = (bboxes[-1],bbox)\n",
    "                bboxes.append(bbox)\n",
    "                break\n",
    "    bboxes[-1] = (bboxes[-1],None)\n",
    "    return list(zip(line_segs, bboxes))\n",
    "        \n",
    "def add_stops(row):\n",
    "    global rows_processed, count\n",
    "    rows_processed +=1\n",
    "    print('Splitting long segments: %.1f' % (100 * rows_processed / total_rows) + '% processed', end='\\r')\n",
    "    line = row['geometry']\n",
    "    dist = line.length\n",
    "    if dist > 300:\n",
    "        if dist > 5000:\n",
    "            segs = math.ceil(dist / 1000)\n",
    "        elif dist > 1500:\n",
    "            segs = math.ceil(dist / 400)\n",
    "        else:\n",
    "            segs = math.ceil(dist / 250)\n",
    "        count += 1\n",
    "        seg_length = dist / segs\n",
    "        return split_line(line, seg_length, segs)\n",
    "    bboxes = [(None,None)]\n",
    "    line_segs = [list(line.coords)]\n",
    "    return list(zip(line_segs, bboxes))\n",
    "\n",
    "unique_seg_to_split['new_segments'] = unique_seg_to_split.apply(add_stops, axis=1)\n",
    "\n",
    "print('Splitting long segments: finished           ')\n",
    "print('Split '+str(count)+' of '+str(total_rows)+' segments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpd.GeoDataFrame(unique_seg_to_split[['start_node','end_node','geometry']],geometry='geometry',crs='EPSG:20255').to_file('tests/unique_seg_to_split.gpkg',driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_unique_seg_to_split = unique_seg_to_split.copy()\n",
    "\n",
    "temp_unique_seg_to_split['STOPA'] = pd.Series([1]*temp_unique_seg_to_split.shape[0],index=temp_unique_seg_to_split.index)\n",
    "temp_unique_seg_to_split['STOPB'] = pd.Series([1]*temp_unique_seg_to_split.shape[0],index=temp_unique_seg_to_split.index)\n",
    "\n",
    "temp_unique_seg_to_split = temp_unique_seg_to_split.drop(['new_segments'],axis=1) \\\n",
    "    .merge(temp_unique_seg_to_split['new_segments'].explode(), right_index = True, left_index = True) \\\n",
    "    .reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_unique_seg_to_split['temp_seq'] = temp_unique_seg_to_split.groupby('unique_seg').cumcount()\n",
    "temp_unique_seg_to_split = temp_unique_seg_to_split.sort_values(['unique_seg','temp_seq'])\n",
    "\n",
    "temp_unique_seg_to_split = temp_unique_seg_to_split.drop(['geometry'], axis=1).rename(columns={'new_segments':'geometry'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_unique_seg_to_split[['geometry','bboxes']] = pd.DataFrame(temp_unique_seg_to_split['geometry'].tolist(), index=temp_unique_seg_to_split.index)\n",
    "temp_unique_seg_to_split[['A_bbox','B_bbox']] = pd.DataFrame(temp_unique_seg_to_split.bboxes.tolist(), index=temp_unique_seg_to_split.index)\n",
    "temp_unique_seg_to_split = temp_unique_seg_to_split.drop(['bboxes'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy nodes\n",
    "\n",
    "max_node = df[['A','B']].max().max()\n",
    "\n",
    "def insert_nodes(group):\n",
    "    global max_node\n",
    "    if group.shape[0] > 1:\n",
    "        new_nodes = list(range(max_node+1,max_node+group.shape[0]))\n",
    "        group['end_node'].iloc[:-1] = new_nodes\n",
    "        group['start_node'].iloc[1:] = new_nodes\n",
    "        max_node = max_node+group.shape[0]-1\n",
    "    return group\n",
    "\n",
    "temp_unique_seg_to_split = temp_unique_seg_to_split.groupby('unique_seg').apply(insert_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_nodes = temp_unique_seg_to_split.loc[temp_unique_seg_to_split['A_bbox'].notnull()].drop_duplicates(subset=['start_node'])[['start_node','geometry','A_bbox']].rename(columns={'start_node':'node','A_bbox':'bbox'})\n",
    "expanded_nodes.geometry = expanded_nodes.geometry.apply(lambda line: Point(line[0]))\n",
    "temp_B_nodes = temp_unique_seg_to_split.loc[temp_unique_seg_to_split['B_bbox'].notnull()].drop_duplicates(subset=['end_node'])[['end_node','geometry','B_bbox']].rename(columns={'end_node':'node','B_bbox':'bbox'})\n",
    "temp_B_nodes.geometry = temp_B_nodes.geometry.apply(lambda line: Point(line[-1]))\n",
    "expanded_nodes = expanded_nodes.append(temp_B_nodes.loc[~temp_B_nodes['node'].isin(expanded_nodes['node'].to_list())])\n",
    "expanded_nodes = expanded_nodes.append(original_nodes.loc[~original_nodes['node'].isin(expanded_nodes['node'].to_list())])\n",
    "expanded_nodes = gpd.GeoDataFrame(expanded_nodes, geometry='geometry')\n",
    "expanded_nodes['node'] = expanded_nodes['node'].apply(int)\n",
    "\n",
    "expanded_nodes = expanded_nodes.sort_values('node').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpd.GeoDataFrame(expanded_nodes[['node','geometry']],geometry='geometry',crs='EPSG:20255').to_file('tests/expanded_nodes.gpkg',driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_nodes = expanded_nodes.loc[~expanded_nodes['node'].isin(original_nodes['node'].to_list())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = new_nodes.bbox.apply(lambda row: list(expanded_nodes.sindex.intersection(row)))\n",
    "tmp = tmp.apply(lambda x: sorted(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_nodes = new_nodes.merge(tmp.explode().rename('near_node_idx'), how='left', left_index=True, right_index=True)\n",
    "dup_nodes = dup_nodes.merge(expanded_nodes[['node','geometry']], how='left', left_on='near_node_idx', right_index=True, suffixes=('','_nearby')).drop(['bbox','geometry','near_node_idx'],axis=1)\n",
    "dup_nodes = dup_nodes.loc[dup_nodes['node'] != dup_nodes['node_nearby']].sort_values(['node','node_nearby'])\n",
    "replace_nodes = dup_nodes.groupby('node').first().reset_index().rename(columns={'node_nearby':'replace_with_node','geometry_nearby':'replace_with_geometry'})\n",
    "\n",
    "replace_nodes.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_been_replaced = {}\n",
    "\n",
    "def replaced(row):\n",
    "    global has_been_replaced\n",
    "    if row.replace_with_node not in has_been_replaced:\n",
    "        has_been_replaced[row.node] = row.replace_with_node\n",
    "    else:\n",
    "        row.replace_with_node = has_been_replaced[row.replace_with_node]\n",
    "        has_been_replaced[row.node] = row.replace_with_node\n",
    "    return row\n",
    "    \n",
    "replace_nodes = replace_nodes.sort_values('node').apply(replaced, axis=1)\n",
    "replace_nodes = replace_nodes.loc[replace_nodes['node'] != replace_nodes['replace_with_node']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpd.GeoDataFrame(replace_nodes,geometry='replace_with_geometry',crs='EPSG:20255').to_file('tests/replace_nodes.gpkg',driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_nodes.replace_with_geometry = replace_nodes.replace_with_geometry.apply(lambda geom: (geom.x,geom.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_geometry(row, node):\n",
    "    if node == 'start_node':\n",
    "        row.geometry[0] = row.replace_with_geometry\n",
    "    elif node == 'end_node':\n",
    "        row.geometry[-1] = row.replace_with_geometry\n",
    "    return row.geometry\n",
    "\n",
    "temp_unique_seg_to_split = temp_unique_seg_to_split.merge(replace_nodes, how='left', left_on='start_node', right_on='node').drop(['node'],axis=1)\n",
    "temp_unique_seg_to_split.loc[temp_unique_seg_to_split['replace_with_node'].notnull(),'start_node'] = temp_unique_seg_to_split.loc[temp_unique_seg_to_split['replace_with_node'].notnull(),'replace_with_node']\n",
    "temp_unique_seg_to_split.loc[temp_unique_seg_to_split['replace_with_node'].notnull(),'geometry'] = temp_unique_seg_to_split.loc[temp_unique_seg_to_split['replace_with_node'].notnull()].apply(lambda row: update_geometry(row, 'start_node'), axis=1)\n",
    "\n",
    "temp_unique_seg_to_split = temp_unique_seg_to_split.drop(['replace_with_node','replace_with_geometry'],axis=1).merge(replace_nodes, how='left', left_on='end_node', right_on='node').drop(['node'],axis=1)\n",
    "temp_unique_seg_to_split.loc[temp_unique_seg_to_split['replace_with_node'].notnull(),'end_node'] = temp_unique_seg_to_split.loc[temp_unique_seg_to_split['replace_with_node'].notnull(),'replace_with_node']\n",
    "temp_unique_seg_to_split.loc[temp_unique_seg_to_split['replace_with_node'].notnull(),'geometry'] = temp_unique_seg_to_split.loc[temp_unique_seg_to_split['replace_with_node'].notnull()].apply(lambda row: update_geometry(row, 'end_node'), axis=1)\n",
    "\n",
    "temp_unique_seg_to_split.geometry = temp_unique_seg_to_split.geometry.apply(LineString)\n",
    "\n",
    "temp_unique_seg_to_split = temp_unique_seg_to_split.drop(['start_point','end_point','A_bbox','B_bbox','replace_with_node','replace_with_geometry'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpd.GeoDataFrame(temp_unique_seg_to_split,geometry='geometry',crs='EPSG:20255').to_file('tests/temp_unique_seg_to_split.gpkg',driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df_unique.merge(temp_unique_seg_to_split, how='left', on='unique_seg', suffixes=('','_merged')).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique.loc[df_unique['start_node_merged'].notnull(),'start_node'] = df_unique.loc[df_unique['start_node_merged'].notnull(),'start_node_merged']\n",
    "df_unique.loc[df_unique['end_node_merged'].notnull(),'end_node'] = df_unique.loc[df_unique['end_node_merged'].notnull(),'end_node_merged']\n",
    "df_unique.loc[df_unique['start_node_merged'].notnull(),'STOPA'] = df_unique.loc[df_unique['start_node_merged'].notnull(),'STOPA_merged']\n",
    "df_unique.loc[df_unique['end_node_merged'].notnull(),'STOPB'] = df_unique.loc[df_unique['end_node_merged'].notnull(),'STOPB_merged']\n",
    "\n",
    "df_unique.loc[df_unique['geometry_merged'].notnull(),'geometry'] = df_unique.loc[df_unique['geometry_merged'].notnull(),'geometry_merged']\n",
    "df_unique = df_unique.drop(['start_node_merged','end_node_merged','STOPA_merged','STOPB_merged','geometry_merged'], axis=1)\n",
    "\n",
    "df_unique['LINKSEQ'] = df_unique.sort_values(['UNIQUE_ROUTE','LINKSEQ','temp_seq']).groupby('UNIQUE_ROUTE').cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpd.GeoDataFrame(df_unique[['A','B','geometry']],geometry='geometry',crs='EPSG:20255').to_file('tests/df_unique.gpkg',driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasts = df_unique.reset_index().groupby('UNIQUE_ROUTE').last().reset_index().set_index('index')\n",
    "lasts['last'] = True\n",
    "\n",
    "df_unique = df_unique.merge(lasts['last'], how='left', left_index=True, right_index=True)\n",
    "\n",
    "df_unique['stop_flag'] = False\n",
    "df_unique.loc[((df_unique['STOPB'] == 1) & (~df_unique['LINKC_AM'].isin([18,19,20,25])) & (df_unique['BUS_SPD'] != 80)) | df_unique['B'].isin(original_nodes['node'].to_list()) | df_unique['last'] == True, 'stop_flag'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidating links and nodes: finished           \n"
     ]
    }
   ],
   "source": [
    "links = []\n",
    "nodes = []\n",
    "total_rows = df_unique.shape[0]\n",
    "rows_processed = 0\n",
    "\n",
    "def final_consolidate_links_and_nodes(row):\n",
    "    global links, nodes, rows_processed\n",
    "    a_node = row['start_node']\n",
    "    b_node = row['end_node']\n",
    "    stopa = 0\n",
    "    stopb = 0\n",
    "    totaldist = np.nan\n",
    "    alllinks = np.nan\n",
    "    allnodes = np.nan\n",
    "    rows_processed +=1\n",
    "    print('Consolidating links and nodes: %.1f' % (100 * rows_processed / total_rows) + '% processed', end='\\r')\n",
    "    new_line = row['geometry']\n",
    "    line_slice = new_line.coords\n",
    "    if len(links) > 0:\n",
    "        line_slice = new_line.coords[1:]\n",
    "    links += [l for l in line_slice]\n",
    "    if len(nodes) == 0:\n",
    "        nodes.append((int(a_node),Point(links[0])))\n",
    "    new_dist = new_line.length\n",
    "    if row['stop_flag'] == True:\n",
    "        alllinks = LineString(links)\n",
    "        totaldist = alllinks.length\n",
    "        nodes.append((int(b_node),Point(links[-1])))\n",
    "        stopa = 1\n",
    "        stopb = 1\n",
    "        allnodes = nodes.copy()\n",
    "        links = []\n",
    "        nodes = []\n",
    "    return [stopa, stopb, totaldist, alllinks, allnodes, [Point(new_line.coords[0]),Point(new_line.coords[-1])], new_dist]\n",
    "\n",
    "df_unique[['STOPA','STOPB','SEGDIST','geometry','SEGNODES','SEGSTOPS','shape_dist_traveled']] = df_unique[['UNIQUE_ROUTE','LINKSEQ','UNIQUE_ROUTE_SEQ','start_node','end_node','geometry','stop_flag']].sort_values(['UNIQUE_ROUTE','LINKSEQ']).apply(lambda row: pd.Series(final_consolidate_links_and_nodes(row)), axis=1)\n",
    "\n",
    "print('Consolidating links and nodes: finished           ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df_unique.loc[df_unique['SEGDIST'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_B</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>STOPA</th>\n",
       "      <th>STOPB</th>\n",
       "      <th>LINKC_AM</th>\n",
       "      <th>BUS_SPD</th>\n",
       "      <th>first</th>\n",
       "      <th>split_line</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>...</th>\n",
       "      <th>end_point</th>\n",
       "      <th>unique_seg</th>\n",
       "      <th>split_line_merged</th>\n",
       "      <th>LINKC_AM_merged</th>\n",
       "      <th>SEGDIST_merged</th>\n",
       "      <th>BUS_SPD_merged</th>\n",
       "      <th>temp_seq</th>\n",
       "      <th>last</th>\n",
       "      <th>stop_flag</th>\n",
       "      <th>shape_dist_traveled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [A_B, A, B, STOPA, STOPB, LINKC_AM, BUS_SPD, first, split_line, DISTANCE, UNIQUE_ROUTE, UNIQUE_ROUTE_SEQ, LINKSEQ, mode, geometry, UNIQUE_ROUTE_first_split, B_first_split, next_UNIQUE_ROUTE, prev_UNIQUE_ROUTE, next_B, prev_B, A_equals_B, SEGDIST, SEGSTOPS, SEGNODES, start_node, end_node, start_point, end_point, unique_seg, split_line_merged, LINKC_AM_merged, SEGDIST_merged, BUS_SPD_merged, temp_seq, last, stop_flag, shape_dist_traveled]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 38 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique.loc[df_unique['STOPA'] != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique['A'] = df_unique['SEGNODES'].apply(lambda nodes: int(nodes[0][0]))\n",
    "df_unique['B'] = df_unique['SEGNODES'].apply(lambda nodes: int(nodes[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_points(group):\n",
    "    first_line = group.iloc[0]\n",
    "    first_line['SEGSTOPS'] = first_line['SEGSTOPS'][0]\n",
    "    first_line['shape_dist_traveled'] = 0\n",
    "    group['SEGSTOPS'] = group['SEGSTOPS'].apply(lambda x: x[1])\n",
    "    group['shape_dist_traveled'] = (group['shape_dist_traveled'].cumsum()*100).apply(round)/100\n",
    "    group['LINKSEQ'] = group['LINKSEQ'] + 1\n",
    "    return first_line.to_frame().T.append(group)\n",
    "\n",
    "df_unique = df_unique.sort_values(['UNIQUE_ROUTE','LINKSEQ'])\n",
    "df_shapes = df_unique[['UNIQUE_ROUTE','LINKSEQ','SEGSTOPS','shape_dist_traveled']]\n",
    "df_shapes = df_shapes.groupby('UNIQUE_ROUTE').apply(expand_points).reset_index(drop=True)\n",
    "\n",
    "df_shapes = gpd.GeoDataFrame(df_shapes, geometry='SEGSTOPS', crs='epsg:20255').to_crs('epsg:4326')\n",
    "\n",
    "df_shapes['shape_pt_lon'],df_shapes['shape_pt_lat'] = zip(*df_shapes['SEGSTOPS'].apply(lambda x: [x.x,x.y]))\n",
    "\n",
    "df_shapes = df_shapes.rename({'LINKDIST':'shape_dist_traveled','LINKSEQ':'shape_pt_sequence'}, axis=1).drop(['SEGSTOPS'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(['year','LINENO','PERIOD','LINKSEQ']).drop(['A','B','STOPA','STOPB','LINKSEQ','geometry'], axis=1).merge(df_unique[['UNIQUE_ROUTE_SEQ','A','B','STOPA','STOPB','SEGDIST','geometry','SEGNODES','LINKSEQ']], how='left', on='UNIQUE_ROUTE_SEQ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3152530"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[(df.STOPA == 1) | (df.STOPB == 1)]\n",
    "\n",
    "df['STOPA_shift'] = df['STOPA'].shift()\n",
    "df['A_shift'] = df['A'].shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>LINENO</th>\n",
       "      <th>LONGNAME</th>\n",
       "      <th>YEAR_LINE_PERIOD</th>\n",
       "      <th>YEAR_LINE</th>\n",
       "      <th>A_B</th>\n",
       "      <th>LINKC_AM</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>BUS_SPD</th>\n",
       "      <th>UNIQUE_ROUTE</th>\n",
       "      <th>...</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>STOPA</th>\n",
       "      <th>STOPB</th>\n",
       "      <th>SEGDIST</th>\n",
       "      <th>geometry</th>\n",
       "      <th>SEGNODES</th>\n",
       "      <th>LINKSEQ</th>\n",
       "      <th>STOPA_shift</th>\n",
       "      <th>A_shift</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th>NAME</th>\n",
       "      <th>PERIOD</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [mode, LINENO, LONGNAME, YEAR_LINE_PERIOD, YEAR_LINE, A_B, LINKC_AM, DISTANCE, BUS_SPD, UNIQUE_ROUTE, UNIQUE_ROUTE_SEQ, A, B, STOPA, STOPB, SEGDIST, geometry, SEGNODES, LINKSEQ, STOPA_shift, A_shift]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 21 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasts = df.groupby(['year','NAME','PERIOD']).last()\n",
    "lasts.loc[lasts['STOPB'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>LINENO</th>\n",
       "      <th>LONGNAME</th>\n",
       "      <th>YEAR_LINE_PERIOD</th>\n",
       "      <th>YEAR_LINE</th>\n",
       "      <th>A_B</th>\n",
       "      <th>LINKC_AM</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>BUS_SPD</th>\n",
       "      <th>UNIQUE_ROUTE</th>\n",
       "      <th>...</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>STOPA</th>\n",
       "      <th>STOPB</th>\n",
       "      <th>SEGDIST</th>\n",
       "      <th>geometry</th>\n",
       "      <th>SEGNODES</th>\n",
       "      <th>LINKSEQ</th>\n",
       "      <th>STOPA_shift</th>\n",
       "      <th>A_shift</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th>NAME</th>\n",
       "      <th>PERIOD</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [mode, LINENO, LONGNAME, YEAR_LINE_PERIOD, YEAR_LINE, A_B, LINKC_AM, DISTANCE, BUS_SPD, UNIQUE_ROUTE, UNIQUE_ROUTE_SEQ, A, B, STOPA, STOPB, SEGDIST, geometry, SEGNODES, LINKSEQ, STOPA_shift, A_shift]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 21 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firsts = df.groupby(['year','NAME','PERIOD']).first()\n",
    "firsts.loc[firsts['STOPA'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['A'] = np.where(df['STOPA'] == 0, df['A_shift'], df['A'])\n",
    "df['STOPA'] = np.where(df['STOPA'] == 0, df['STOPA_shift'], df['STOPA'])\n",
    "\n",
    "df = df.loc[df.STOPB == 1].sort_values(['year','LINENO','PERIOD','LINKSEQ'])\n",
    "df['A'] = df['A'].apply(int)\n",
    "df['B'] = df['B'].apply(int)\n",
    "\n",
    "df = df.drop(['LINKSEQ','UNIQUE_ROUTE_SEQ'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERIOD</th>\n",
       "      <th>mode</th>\n",
       "      <th>LINENO</th>\n",
       "      <th>NAME</th>\n",
       "      <th>LONGNAME</th>\n",
       "      <th>year</th>\n",
       "      <th>YEAR_LINE_PERIOD</th>\n",
       "      <th>YEAR_LINE</th>\n",
       "      <th>A_B</th>\n",
       "      <th>LINKC_AM</th>\n",
       "      <th>...</th>\n",
       "      <th>UNIQUE_ROUTE</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>STOPA</th>\n",
       "      <th>STOPB</th>\n",
       "      <th>SEGDIST</th>\n",
       "      <th>geometry</th>\n",
       "      <th>SEGNODES</th>\n",
       "      <th>STOPA_shift</th>\n",
       "      <th>A_shift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [PERIOD, mode, LINENO, NAME, LONGNAME, year, YEAR_LINE_PERIOD, YEAR_LINE, A_B, LINKC_AM, DISTANCE, BUS_SPD, UNIQUE_ROUTE, A, B, STOPA, STOPB, SEGDIST, geometry, SEGNODES, STOPA_shift, A_shift]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 22 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['A'] == df['B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpd.GeoDataFrame(df[['A','B','geometry']],geometry='geometry',crs='EPSG:20255').to_file('tests/df.gpkg',driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR_LINE</th>\n",
       "      <th>year</th>\n",
       "      <th>id</th>\n",
       "      <th>period</th>\n",
       "      <th>route</th>\n",
       "      <th>name</th>\n",
       "      <th>mode</th>\n",
       "      <th>headway</th>\n",
       "      <th>speed</th>\n",
       "      <th>UNIQUE_ROUTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018_305</td>\n",
       "      <td>2018</td>\n",
       "      <td>305</td>\n",
       "      <td>AM</td>\n",
       "      <td>109</td>\n",
       "      <td>BOX HILL - PORT MELBOURNE</td>\n",
       "      <td>tram</td>\n",
       "      <td>7.5</td>\n",
       "      <td>15.42</td>\n",
       "      <td>1454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018_305</td>\n",
       "      <td>2018</td>\n",
       "      <td>305</td>\n",
       "      <td>IP</td>\n",
       "      <td>109</td>\n",
       "      <td>BOX HILL - PORT MELBOURNE</td>\n",
       "      <td>tram</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.24</td>\n",
       "      <td>1454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018_305</td>\n",
       "      <td>2018</td>\n",
       "      <td>305</td>\n",
       "      <td>OP</td>\n",
       "      <td>109</td>\n",
       "      <td>BOX HILL - PORT MELBOURNE</td>\n",
       "      <td>tram</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.54</td>\n",
       "      <td>1454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018_305</td>\n",
       "      <td>2018</td>\n",
       "      <td>305</td>\n",
       "      <td>PM</td>\n",
       "      <td>109</td>\n",
       "      <td>BOX HILL - PORT MELBOURNE</td>\n",
       "      <td>tram</td>\n",
       "      <td>7.5</td>\n",
       "      <td>15.98</td>\n",
       "      <td>1454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018_2</td>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>AM</td>\n",
       "      <td>11011</td>\n",
       "      <td>Mernda - Flinders Street - All Stations</td>\n",
       "      <td>train</td>\n",
       "      <td>13.3</td>\n",
       "      <td>36.16</td>\n",
       "      <td>782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31625</th>\n",
       "      <td>2051_1476</td>\n",
       "      <td>2051</td>\n",
       "      <td>1476</td>\n",
       "      <td>PM</td>\n",
       "      <td>WredstoneR</td>\n",
       "      <td>Sunbury - Redstone Hill</td>\n",
       "      <td>bus</td>\n",
       "      <td>40.0</td>\n",
       "      <td>20.09</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31626</th>\n",
       "      <td>2051_1475</td>\n",
       "      <td>2051</td>\n",
       "      <td>1475</td>\n",
       "      <td>AM</td>\n",
       "      <td>Wredstone</td>\n",
       "      <td>Sunbury - Redstone Hill</td>\n",
       "      <td>bus</td>\n",
       "      <td>40.0</td>\n",
       "      <td>21.36</td>\n",
       "      <td>1304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31627</th>\n",
       "      <td>2051_1475</td>\n",
       "      <td>2051</td>\n",
       "      <td>1475</td>\n",
       "      <td>IP</td>\n",
       "      <td>Wredstone</td>\n",
       "      <td>Sunbury - Redstone Hill</td>\n",
       "      <td>bus</td>\n",
       "      <td>40.0</td>\n",
       "      <td>21.54</td>\n",
       "      <td>1304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31628</th>\n",
       "      <td>2051_1475</td>\n",
       "      <td>2051</td>\n",
       "      <td>1475</td>\n",
       "      <td>OP</td>\n",
       "      <td>Wredstone</td>\n",
       "      <td>Sunbury - Redstone Hill</td>\n",
       "      <td>bus</td>\n",
       "      <td>60.0</td>\n",
       "      <td>21.72</td>\n",
       "      <td>1304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31629</th>\n",
       "      <td>2051_1475</td>\n",
       "      <td>2051</td>\n",
       "      <td>1475</td>\n",
       "      <td>PM</td>\n",
       "      <td>Wredstone</td>\n",
       "      <td>Sunbury - Redstone Hill</td>\n",
       "      <td>bus</td>\n",
       "      <td>40.0</td>\n",
       "      <td>19.01</td>\n",
       "      <td>1304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31630 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       YEAR_LINE  year    id period       route  \\\n",
       "0       2018_305  2018   305     AM         109   \n",
       "1       2018_305  2018   305     IP         109   \n",
       "2       2018_305  2018   305     OP         109   \n",
       "3       2018_305  2018   305     PM         109   \n",
       "4         2018_2  2018     2     AM       11011   \n",
       "...          ...   ...   ...    ...         ...   \n",
       "31625  2051_1476  2051  1476     PM  WredstoneR   \n",
       "31626  2051_1475  2051  1475     AM   Wredstone   \n",
       "31627  2051_1475  2051  1475     IP   Wredstone   \n",
       "31628  2051_1475  2051  1475     OP   Wredstone   \n",
       "31629  2051_1475  2051  1475     PM   Wredstone   \n",
       "\n",
       "                                          name   mode  headway  speed  \\\n",
       "0                    BOX HILL - PORT MELBOURNE   tram      7.5  15.42   \n",
       "1                    BOX HILL - PORT MELBOURNE   tram     10.0  16.24   \n",
       "2                    BOX HILL - PORT MELBOURNE   tram     12.0  16.54   \n",
       "3                    BOX HILL - PORT MELBOURNE   tram      7.5  15.98   \n",
       "4      Mernda - Flinders Street - All Stations  train     13.3  36.16   \n",
       "...                                        ...    ...      ...    ...   \n",
       "31625                  Sunbury - Redstone Hill    bus     40.0  20.09   \n",
       "31626                  Sunbury - Redstone Hill    bus     40.0  21.36   \n",
       "31627                  Sunbury - Redstone Hill    bus     40.0  21.54   \n",
       "31628                  Sunbury - Redstone Hill    bus     60.0  21.72   \n",
       "31629                  Sunbury - Redstone Hill    bus     40.0  19.01   \n",
       "\n",
       "       UNIQUE_ROUTE  \n",
       "0              1454  \n",
       "1              1454  \n",
       "2              1454  \n",
       "3              1454  \n",
       "4               782  \n",
       "...             ...  \n",
       "31625           424  \n",
       "31626          1304  \n",
       "31627          1304  \n",
       "31628          1304  \n",
       "31629          1304  \n",
       "\n",
       "[31630 rows x 10 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_reporting['Time Period'] = pt_reporting['Time Period'].apply(lambda x: x.split(' ')[0])\n",
    "pt_reporting['YEAR_NAME_PERIOD'] = pt_reporting['year'] + '_' + pt_reporting['Short Name'] + \"_\" + pt_reporting['Time Period']\n",
    "\n",
    "df['YEAR_NAME_PERIOD'] = df['year'] + '_' + df['NAME'] + \"_\" + df['PERIOD']\n",
    "unique_speeds_headways = df.groupby('YEAR_NAME_PERIOD').first().reset_index()\n",
    "unique_speeds_headways = unique_speeds_headways.merge(pt_reporting[['YEAR_NAME_PERIOD','Headway','Average Speed']], how='left', on='YEAR_NAME_PERIOD').drop('YEAR_NAME_PERIOD', axis=1)[['YEAR_LINE','year','LINENO','PERIOD','NAME','LONGNAME','mode','Headway','Average Speed','UNIQUE_ROUTE']]\n",
    "unique_speeds_headways = unique_speeds_headways.rename({'LINENO':'id','NAME':'route','LONGNAME':'name','PERIOD':'period','Average Speed':'speed','Headway':'headway'}, axis=1)\n",
    "\n",
    "unique_speeds_headways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_missing_speeds(row):\n",
    "    for i in range(len(speed_cols)):\n",
    "        if row[speed_cols[i]] == 0:\n",
    "            row[speed_cols[i]] = row[speed_cols[(i+3) % 4]]\n",
    "    return row\n",
    "\n",
    "pivot = pd.pivot_table(unique_speeds_headways, index=['YEAR_LINE'], columns=[\"period\"], values=[\"headway\",\"speed\"], aggfunc=np.mean)\n",
    "pivot.columns = ['_'.join(col).strip().lower() for col in pivot.columns.values]\n",
    "\n",
    "speed_cols = ['speed_am','speed_ip','speed_pm','speed_op']\n",
    "headway_cols = ['headway_am','headway_ip','headway_pm','headway_op']\n",
    "unique_speeds_headways = unique_speeds_headways.merge(pivot.reset_index(), how='left', on='YEAR_LINE').groupby('YEAR_LINE').first().reset_index().replace(np.nan,0).drop('period',axis=1)\n",
    "unique_speeds_headways = unique_speeds_headways.apply(fix_missing_speeds, axis=1)\n",
    "unique_speeds_headways = unique_speeds_headways.loc[(unique_speeds_headways[headway_cols] != 0).any(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: name, dtype: object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stop_nodes(group):\n",
    "    global stop_list, link_list, dist_list, node_list, counter\n",
    "    counter = 0\n",
    "    link_list = []\n",
    "    stop_list = []\n",
    "    dist_list = []\n",
    "    node_list = []\n",
    "    group.apply(get_nodes, axis=1)\n",
    "    return [list(zip(link_list, node_list, dist_list)), MultiLineString(link_list), stop_list]\n",
    "    \n",
    "def get_nodes(row):\n",
    "    global stop_list, link_list, dist_list, node_list, counter\n",
    "    link_list.append(row['geometry'])\n",
    "    node_list.append(str(row['A'])+'_'+str(row['B']))\n",
    "    dist_list.append(row['SEGDIST'])\n",
    "    if counter == 0:\n",
    "        stop_list.append(row['SEGNODES'][0])\n",
    "    stop_list.append(row['SEGNODES'][1])\n",
    "    counter += 1\n",
    "    \n",
    "def number_segments(l):\n",
    "    segs = ''\n",
    "    for i in range(len(l)):\n",
    "        segs += str(i)+','\n",
    "    return segs[:-1]\n",
    "\n",
    "stop_patterns = df.loc[df['YEAR_LINE_PERIOD'].isin(unique_routes['YEAR_LINE_PERIOD'].to_list())].groupby('UNIQUE_ROUTE').apply(stop_nodes).reset_index()\n",
    "stop_patterns[['SEGMENTS','LINE','STOPPATTERN']] = pd.DataFrame(stop_patterns[0].tolist(), index=stop_patterns.index)\n",
    "stop_patterns = stop_patterns.drop(0, axis=1)\n",
    "stop_patterns['segments'] = stop_patterns['SEGMENTS'].apply(number_segments)\n",
    "\n",
    "stop_patterns = unique_speeds_headways.merge(stop_patterns, how='left', on='UNIQUE_ROUTE')\n",
    "\n",
    "# Replace non-ASCII characters\n",
    "table = {0x2013: '-', 0x2014: '--', 0x00a7: 'sect. ', 0x00A0: ' '}\n",
    "stop_patterns.loc[~stop_patterns['name'].apply(lambda string: string.isascii()),'name'] = stop_patterns.loc[~stop_patterns['name'].apply(lambda string: string.isascii()),'name'].apply(lambda string: string.translate(table))\n",
    "stop_patterns.loc[~stop_patterns['name'].apply(lambda string: string.isascii()),'name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_output = stop_patterns.groupby('YEAR_LINE').first().reset_index()[['year','id','route','mode','name','LINE']]\n",
    "route_output = route_output.rename({'LINE':'geometry','mode':'OPERATOR_N','route':'ROUTE_SHORT','name':'ROUTE_LONG','id':'ROUTE_ID'}, axis=1)\n",
    "route_output['SHAPE_ID'] = route_output['ROUTE_ID']\n",
    "\n",
    "route_output = gpd.GeoDataFrame(route_output[['year','SHAPE_ID','ROUTE_ID','ROUTE_SHORT','ROUTE_LONG','OPERATOR_N','geometry']], geometry='geometry', crs='epsg:20255').to_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes = stop_patterns[['year','route','id','mode','name']+headway_cols+speed_cols+['segments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_expand = stop_patterns[['id','year','mode','route','name','UNIQUE_ROUTE']] \\\n",
    "    .merge(stop_patterns.SEGMENTS.explode(), right_index = True, left_index = True) \\\n",
    "    .dropna() \\\n",
    "    .sort_values(['year','id']) \\\n",
    "    .reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_expand[['SEGMENT','NODES','route_dist']] = pd.DataFrame(seg_expand['SEGMENTS'].tolist(), index=seg_expand.index)\n",
    "seg_expand = gpd.GeoDataFrame(seg_expand.drop('SEGMENTS', axis=1).rename(columns={'SEGMENT':'geometry'}), geometry='geometry', crs='epsg:20255').to_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_expand['NODES'] = seg_expand['NODES'].apply(lambda x: x.split('_'))\n",
    "seg_expand[['stop1','stop2']] = pd.DataFrame(seg_expand['NODES'].tolist(), index=seg_expand.index)\n",
    "seg_expand = seg_expand.drop('NODES', axis=1)\n",
    "seg_expand['stop1N'] = seg_expand['stop1'].apply(str)\n",
    "seg_expand['stop2N'] = seg_expand['stop2'].apply(str)\n",
    "seg_expand = seg_expand.rename({'id':'routes'}, axis=1)\n",
    "seg_expand['id'] = seg_expand.groupby(['year','routes']).cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "False    483512\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(seg_expand['stop1'] == seg_expand['stop2']).to_frame().groupby(0)[0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_expand = stop_patterns[['id','year','mode','route','name']] \\\n",
    "    .merge(stop_patterns.STOPPATTERN.explode(), right_index = True, left_index = True) \\\n",
    "    .dropna() \\\n",
    "    .sort_values(['year','id']) \\\n",
    "    .reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_expand[['id','STOPPATTERN']] = pd.DataFrame(sp_expand['STOPPATTERN'].tolist(), index=sp_expand.index)\n",
    "sp_expand['name'] = sp_expand['id'].apply(str)\n",
    "sp_expand = gpd.GeoDataFrame(sp_expand.rename(columns={'STOPPATTERN':'geometry'}), geometry='geometry', crs='epsg:20255').to_crs('epsg:4326')\n",
    "sp_expand = sp_expand.drop_duplicates(subset=['id']).drop(['year','route','mode'], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating GTFS outputs for 2018 reference case\n",
      "\n",
      "Creating GTFS feed for bus\n",
      "Processing routes 0 to 907\n",
      "About to save complete timetable to file output/4/google_transit.zip ...\n",
      "...finished writing to file output/4/google_transit.zip.\n",
      "Creating GTFS feed for skybus\n",
      "Processing routes 0 to 17\n",
      "About to save complete timetable to file output/11/google_transit.zip ...\n",
      "...finished writing to file output/11/google_transit.zip.\n",
      "Creating GTFS feed for train\n",
      "Processing routes 0 to 267\n",
      "About to save complete timetable to file output/2/google_transit.zip ...\n",
      "...finished writing to file output/2/google_transit.zip.\n",
      "Creating GTFS feed for tram\n",
      "Processing routes 0 to 45\n",
      "About to save complete timetable to file output/3/google_transit.zip ...\n",
      "...finished writing to file output/3/google_transit.zip.\n",
      "Creating GTFS feed for vline\n",
      "Processing routes 0 to 95\n",
      "About to save complete timetable to file output/1/google_transit.zip ...\n",
      "...finished writing to file output/1/google_transit.zip.\n",
      "\n",
      "Creating output/VITM_2018_GTFS.zip...\n",
      "\n",
      "Generating GTFS outputs for 2026 reference case\n",
      "\n",
      "Creating GTFS feed for bus\n",
      "Processing routes 0 to 1258\n",
      "About to save complete timetable to file output/4/google_transit.zip ...\n",
      " column route_long_name\n",
      "Invalid value b'pearcedale - frankston' in field b'route_long_name'\n",
      "b'The same combination of route_short_name and route_long_name shouldn\\'t be used for more than one route, as it is for the for the two routes with IDs \"894\" and \"892\".'\n",
      "...finished writing to file output/4/google_transit.zip.\n",
      "Creating GTFS feed for skybus\n",
      "Processing routes 0 to 17\n",
      "About to save complete timetable to file output/11/google_transit.zip ...\n",
      "...finished writing to file output/11/google_transit.zip.\n",
      "Creating GTFS feed for train\n",
      "Processing routes 0 to 60\n",
      "About to save complete timetable to file output/2/google_transit.zip ...\n",
      "...finished writing to file output/2/google_transit.zip.\n",
      "Creating GTFS feed for tram\n",
      "Processing routes 0 to 51\n",
      "About to save complete timetable to file output/3/google_transit.zip ...\n",
      "...finished writing to file output/3/google_transit.zip.\n",
      "Creating GTFS feed for vline\n",
      "Processing routes 0 to 53\n",
      "About to save complete timetable to file output/1/google_transit.zip ...\n",
      "...finished writing to file output/1/google_transit.zip.\n",
      "\n",
      "Creating output/VITM_2026_GTFS.zip...\n",
      "\n",
      "Generating GTFS outputs for 2031 reference case\n",
      "\n",
      "Creating GTFS feed for bus\n",
      "Processing routes 0 to 1258\n",
      "About to save complete timetable to file output/4/google_transit.zip ...\n",
      " column route_long_name\n",
      "Invalid value b'pearcedale - frankston' in field b'route_long_name'\n",
      "b'The same combination of route_short_name and route_long_name shouldn\\'t be used for more than one route, as it is for the for the two routes with IDs \"945\" and \"943\".'\n",
      "...finished writing to file output/4/google_transit.zip.\n",
      "Creating GTFS feed for skybus\n",
      "Processing routes 0 to 17\n",
      "About to save complete timetable to file output/11/google_transit.zip ...\n",
      "...finished writing to file output/11/google_transit.zip.\n",
      "Creating GTFS feed for train\n",
      "Processing routes 0 to 53\n",
      "About to save complete timetable to file output/2/google_transit.zip ...\n",
      "...finished writing to file output/2/google_transit.zip.\n",
      "Creating GTFS feed for tram\n",
      "Processing routes 0 to 51\n",
      "About to save complete timetable to file output/3/google_transit.zip ...\n",
      "...finished writing to file output/3/google_transit.zip.\n",
      "Creating GTFS feed for vline\n",
      "Processing routes 0 to 53\n",
      "About to save complete timetable to file output/1/google_transit.zip ...\n",
      "...finished writing to file output/1/google_transit.zip.\n",
      "\n",
      "Creating output/VITM_2031_GTFS.zip...\n",
      "\n",
      "Generating GTFS outputs for 2036 reference case\n",
      "\n",
      "Creating GTFS feed for bus\n",
      "Processing routes 0 to 1351\n",
      "About to save complete timetable to file output/4/google_transit.zip ...\n",
      " column route_long_name\n",
      "Invalid value b'pearcedale - frankston' in field b'route_long_name'\n",
      "b'The same combination of route_short_name and route_long_name shouldn\\'t be used for more than one route, as it is for the for the two routes with IDs \"770\" and \"768\".'\n",
      "...finished writing to file output/4/google_transit.zip.\n",
      "Creating GTFS feed for skybus\n",
      "Processing routes 0 to 17\n",
      "About to save complete timetable to file output/11/google_transit.zip ...\n",
      "...finished writing to file output/11/google_transit.zip.\n",
      "Creating GTFS feed for train\n",
      "Processing routes 0 to 61\n",
      "About to save complete timetable to file output/2/google_transit.zip ...\n",
      "...finished writing to file output/2/google_transit.zip.\n",
      "Creating GTFS feed for tram\n",
      "Processing routes 0 to 51\n",
      "About to save complete timetable to file output/3/google_transit.zip ...\n",
      "...finished writing to file output/3/google_transit.zip.\n",
      "Creating GTFS feed for vline\n",
      "Processing routes 0 to 51\n",
      "About to save complete timetable to file output/1/google_transit.zip ...\n",
      "...finished writing to file output/1/google_transit.zip.\n",
      "\n",
      "Creating output/VITM_2036_GTFS.zip...\n",
      "\n",
      "Generating GTFS outputs for 2041 reference case\n",
      "\n",
      "Creating GTFS feed for bus\n",
      "Processing routes 0 to 1351\n",
      "About to save complete timetable to file output/4/google_transit.zip ...\n",
      " column route_long_name\n",
      "Invalid value b'pearcedale - frankston' in field b'route_long_name'\n",
      "b'The same combination of route_short_name and route_long_name shouldn\\'t be used for more than one route, as it is for the for the two routes with IDs \"816\" and \"814\".'\n",
      "...finished writing to file output/4/google_transit.zip.\n",
      "Creating GTFS feed for skybus\n",
      "Processing routes 0 to 17\n",
      "About to save complete timetable to file output/11/google_transit.zip ...\n",
      "...finished writing to file output/11/google_transit.zip.\n",
      "Creating GTFS feed for train\n",
      "Processing routes 0 to 65\n",
      "About to save complete timetable to file output/2/google_transit.zip ...\n",
      "...finished writing to file output/2/google_transit.zip.\n",
      "Creating GTFS feed for tram\n",
      "Processing routes 0 to 51\n",
      "About to save complete timetable to file output/3/google_transit.zip ...\n",
      "...finished writing to file output/3/google_transit.zip.\n",
      "Creating GTFS feed for vline\n",
      "Processing routes 0 to 41\n",
      "About to save complete timetable to file output/1/google_transit.zip ...\n",
      "...finished writing to file output/1/google_transit.zip.\n",
      "\n",
      "Creating output/VITM_2041_GTFS.zip...\n",
      "\n",
      "Generating GTFS outputs for 2051 reference case\n",
      "\n",
      "Creating GTFS feed for bus\n",
      "Processing routes 0 to 1430\n",
      "About to save complete timetable to file output/4/google_transit.zip ...\n",
      " column route_long_name\n",
      "Invalid value b'pearcedale - frankston' in field b'route_long_name'\n",
      "b'The same combination of route_short_name and route_long_name shouldn\\'t be used for more than one route, as it is for the for the two routes with IDs \"704\" and \"702\".'\n",
      "...finished writing to file output/4/google_transit.zip.\n",
      "Creating GTFS feed for skybus\n",
      "Processing routes 0 to 17\n",
      "About to save complete timetable to file output/11/google_transit.zip ...\n",
      "...finished writing to file output/11/google_transit.zip.\n",
      "Creating GTFS feed for train\n",
      "Processing routes 0 to 67\n",
      "About to save complete timetable to file output/2/google_transit.zip ...\n",
      "...finished writing to file output/2/google_transit.zip.\n",
      "Creating GTFS feed for tram\n",
      "Processing routes 0 to 51\n",
      "About to save complete timetable to file output/3/google_transit.zip ...\n",
      "...finished writing to file output/3/google_transit.zip.\n",
      "Creating GTFS feed for vline\n",
      "Processing routes 0 to 39\n",
      "About to save complete timetable to file output/1/google_transit.zip ...\n",
      "...finished writing to file output/1/google_transit.zip.\n",
      "\n",
      "Creating output/VITM_2051_GTFS.zip...\n",
      "\n",
      "Cleaning up temp files...\n",
      "Done\n",
      "Pre-processing time: 12.6 minutes\n",
      "Total time: 283.4 minutes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_processing_time = time.time()\n",
    "pre_processing_time = end_processing_time - start_time\n",
    "\n",
    "# from importlib import reload\n",
    "# import route_segs\n",
    "# reload(create_gtfs_from_basicinfo)\n",
    "# reload(route_segs)\n",
    "\n",
    "# print('Saving network shapefiles')\n",
    "# if not os.path.isdir('output/Network'):\n",
    "#     os.mkdir('output/Network')\n",
    "# for year in years:\n",
    "#     route_output.loc[route_output['year'] == year].to_file('output/Network/VITM_'+year+'_segments.shp')\n",
    "    \n",
    "#     stop_ids = seg_expand.loc[seg_expand['year'] == year, ['stop1','stop2']].values\n",
    "#     stop_ids = list(dict.fromkeys([int(val) for sublist in stop_ids for val in sublist]))\n",
    "    \n",
    "#     sp_expand.loc[sp_expand['id'].isin(stop_ids)].to_file('output/Network/VITM_'+year+'_stops.shp')\n",
    "\n",
    "if not os.path.isdir('input/temp'):\n",
    "    os.mkdir('input/temp')\n",
    "\n",
    "modes = {'vline':'1','train':'2','tram':'3','bus':'4','skybus':'11'}\n",
    "\n",
    "for year in years:\n",
    "    print('Generating GTFS outputs for '+year+' reference case\\n')\n",
    "    for mode in all_modes:\n",
    "        if mode not in modes:\n",
    "            print(\"'\"+mode+\"' not a valid mode\")\n",
    "            continue\n",
    "#         if mode not in seg_expand['mode'].unique().to_list():\n",
    "#             continue\n",
    "    #     if mode != 'train':\n",
    "    #         continue\n",
    "        sys.argv = ['', '--year', year, '--service', mode]\n",
    "\n",
    "        if not os.path.isdir('output/'+modes[mode]):\n",
    "            os.mkdir('output/'+modes[mode])\n",
    "\n",
    "        stop_ids = seg_expand.loc[(seg_expand['mode'] == mode) & (seg_expand['year'] == year), ['stop1','stop2']].values\n",
    "        stop_ids = list(dict.fromkeys([int(val) for sublist in stop_ids for val in sublist]))\n",
    "        \n",
    "        unique_route_ids = list(seg_expand.loc[(seg_expand['mode'] == mode) & (seg_expand['year'] == year), 'UNIQUE_ROUTE'].unique())\n",
    "\n",
    "        sp_expand.loc[sp_expand['id'].isin(stop_ids)].to_file('input/temp/VITM_stops.shp')\n",
    "        seg_expand.loc[(seg_expand['mode'] == mode) & (seg_expand['year'] == year)].drop('UNIQUE_ROUTE',axis=1).to_file('input/temp/VITM_segments.shp')\n",
    "        routes.loc[(routes['mode'] == mode) & (routes['year'] == year)].to_csv('input/temp/VITM_routes.csv', index=False, sep=';')\n",
    "        df_shapes.loc[df_shapes['UNIQUE_ROUTE'].isin(unique_route_ids)].drop('UNIQUE_ROUTE',axis=1).to_csv('input/temp/shapes.txt', index=False)\n",
    "\n",
    "        create_gtfs_from_basicinfo.run()\n",
    "\n",
    "    print('\\nCreating output/VITM_'+year+'_GTFS.zip...\\n')\n",
    "    # create a ZipFile object\n",
    "    with zipfile.ZipFile('output/VITM_'+year+'_GTFS.zip', 'w', zipfile.ZIP_DEFLATED) as zipObj:\n",
    "        # Iterate over all the files in directory\n",
    "        for folder_id in list(modes.values()):\n",
    "            if os.path.isdir('output/'+folder_id):\n",
    "                for folderName, subfolders, filenames in os.walk('output/'+folder_id):\n",
    "                    for filename in filenames:\n",
    "                        #create complete filepath of file in directory\n",
    "                        filePath = os.path.join(folderName, filename)\n",
    "                        # Add file to zip\n",
    "                        zipObj.write(filePath, os.path.join(folder_id, filename))\n",
    "                shutil.rmtree('output/'+folder_id)\n",
    "    \n",
    "print('Cleaning up temp files...')\n",
    "shutil.rmtree('input/temp')\n",
    "\n",
    "print('Done')\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print('Pre-processing time: %.1f minutes' % (pre_processing_time / 60))\n",
    "print('Total time: %.1f minutes' % (total_time / 60))\n",
    "\n",
    "os.system(\"printf '\\a'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
